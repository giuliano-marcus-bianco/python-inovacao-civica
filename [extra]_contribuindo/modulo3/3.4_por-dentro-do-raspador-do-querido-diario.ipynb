{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 - Por dentro do raspador do Querido Diário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta aula foram tratados os seguintes temas:\n",
    "1. Introdução à raspagem de dados;\n",
    "2. Introdução ao Scrapy;\n",
    "3. Raspador do Querido Diário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Raspagem de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível extrair (*raspar*) o conteúdo apresentado em qualquer site, entendendo como o site serve esse contúdo e replicando isso em um programa (*raspador*) que simula uma interação com este site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introdução ao Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy é um framework eficiente que utiliza o mínimo de recursos necessários para interagir com a maioria dos sites (HTTP). Combinando com sua natureza assíncrona e uma série de baterias inclusas, um projeto Scrapy tem:\n",
    "- Raspadores rápidos de desenvolver;\n",
    "- Raspadores eficientes durante a execução;\n",
    "- Robustez para adaptar as mudanças e crescimento necessários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Instalar e Iniciar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta parte vamos instalar o framework ***Scrapy*** e iniciar um novo projeto. Os comandos para ambos estão abaixo, mas foram comentados por segurança, para não serem executados por engano e instalar fora do ambiente virtual ou iniciar outro projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.9.0-py2.py3-none-any.whl (277 kB)\n",
      "                                              0.0/277.2 kB ? eta -:--:--\n",
      "     ----------------                       122.9/277.2 kB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 277.2/277.2 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting Twisted>=18.9.0 (from scrapy)\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "                                              0.0/3.1 MB ? eta -:--:--\n",
      "     -------                                  0.6/3.1 MB 38.2 MB/s eta 0:00:01\n",
      "     -------------------------------          2.4/3.1 MB 31.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.1/3.1 MB 28.5 MB/s eta 0:00:00\n",
      "Collecting cryptography>=3.4.6 (from scrapy)\n",
      "  Downloading cryptography-41.0.2-cp37-abi3-win_amd64.whl (2.6 MB)\n",
      "                                              0.0/2.6 MB ? eta -:--:--\n",
      "     ----------------------                   1.5/2.6 MB 47.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 33.5 MB/s eta 0:00:00\n",
      "Collecting cssselect>=0.9.1 (from scrapy)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting pyOpenSSL>=21.0.0 (from scrapy)\n",
      "  Downloading pyOpenSSL-23.2.0-py3-none-any.whl (59 kB)\n",
      "                                              0.0/59.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.0/59.0 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting zope.interface>=5.1.0 (from scrapy)\n",
      "  Downloading zope.interface-6.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "                                              0.0/204.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 204.1/204.1 kB 12.9 MB/s eta 0:00:00\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gm_bi\\documents\\github\\escola_de_dados\\python-inovacao-civica\\env\\lib\\site-packages (from scrapy) (65.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\gm_bi\\documents\\github\\escola_de_dados\\python-inovacao-civica\\env\\lib\\site-packages (from scrapy) (23.1)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
      "                                              0.0/93.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 93.3/93.3 kB ? eta 0:00:00\n",
      "Collecting lxml>=4.3.0 (from scrapy)\n",
      "  Using cached lxml-4.9.3-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=3.4.6->scrapy)\n",
      "  Using cached cffi-1.15.1-cp311-cp311-win_amd64.whl (179 kB)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six in c:\\users\\gm_bi\\documents\\github\\escola_de_dados\\python-inovacao-civica\\env\\lib\\site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\gm_bi\\documents\\github\\escola_de_dados\\python-inovacao-civica\\env\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "                                              0.0/83.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 83.9/83.9 kB ? eta 0:00:00\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "                                              0.0/181.3 kB ? eta -:--:--\n",
      "     ------------------------------------- 181.3/181.3 kB 10.7 MB/s eta 0:00:00\n",
      "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=21.3.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "                                              0.0/74.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 74.6/74.6 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\gm_bi\\documents\\github\\escola_de_dados\\python-inovacao-civica\\env\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.7.1)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading twisted_iocpsupport-1.0.3-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\gm_bi\\documents\\github\\escola_de_dados\\python-inovacao-civica\\env\\lib\\site-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\gm_bi\\documents\\github\\escola_de_dados\\python-inovacao-civica\\env\\lib\\site-packages (from tldextract->scrapy) (2.31.0)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract->scrapy)\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=3.4.6->scrapy)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gm_bi\\documents\\github\\escola_de_dados\\python-inovacao-civica\\env\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gm_bi\\documents\\github\\escola_de_dados\\python-inovacao-civica\\env\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gm_bi\\documents\\github\\escola_de_dados\\python-inovacao-civica\\env\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2023.5.7)\n",
      "Installing collected packages: twisted-iocpsupport, PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, pycparser, pyasn1, protego, lxml, jmespath, itemadapter, hyperlink, filelock, cssselect, Automat, Twisted, requests-file, pyasn1-modules, parsel, cffi, tldextract, itemloaders, cryptography, service-identity, pyOpenSSL, scrapy\n",
      "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 cffi-1.15.1 constantly-15.1.0 cryptography-41.0.2 cssselect-1.2.0 filelock-3.12.2 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 lxml-4.9.3 parsel-1.8.1 protego-0.2.1 pyOpenSSL-23.2.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycparser-2.21 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.9.0 service-identity-23.1.0 tldextract-3.4.4 twisted-iocpsupport-1.0.3 w3lib-2.1.1 zope.interface-6.0\n"
     ]
    }
   ],
   "source": [
    "# instalar framework Scrapy\n",
    "# !pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'projeto_inovacao', using template directory 'C:\\Users\\gm_bi\\Documents\\GitHub\\Escola_de_Dados\\python-inovacao-civica\\env\\Lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    C:\\Users\\gm_bi\\Documents\\GitHub\\Escola_de_Dados\\python-inovacao-civica\\[extra]_contribuindo\\modulo3\\projeto_inovacao\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd projeto_inovacao\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "# iniciar projeto\n",
    "# !scrapy startproject projeto_inovacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Raspador do Querido Diário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de agora vamos criar um raspador para a página do Diário Oficial da cidade de Natal (RN). Para isso, é criado o arquivo [*rn_natal.py*](./projeto_inovacao/projeto_inovacao/spiders/rn_natal.py) na pasta [*spiders*](./projeto_inovacao/projeto_inovacao/spiders/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um raspador Scrapy é uma classe, logo, nesse arquivo vamos definir a seguinte classe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class RnNatalSpider(scrapy.Spider): # Spider base: faz com que a classe RnNatalSpider herde tudo que é necessário de scrapy.Spider\n",
    "    name = \"rn_natal\" # todo raspador Scrapy é excecutado a partir do seu nome\n",
    "    start_urls = \"https://www.natal.rn.gov.br/dom/\" # primeira requisição\n",
    "    \n",
    "    def parse(self, response): # a resposta da requisição da url vai para o método padrão parse\n",
    "        yield {\"body\": response.text} # yield é como um return que permite retornar mais de uma vez na mesma função"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Executar raspador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para executar o raspador, rode o seguinte comando **a partir da pasta onde está o arquivo *scrapy.cfg*** e o conteúdo será salvo no arquivo ***output.json***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl rn_natal -o output.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
