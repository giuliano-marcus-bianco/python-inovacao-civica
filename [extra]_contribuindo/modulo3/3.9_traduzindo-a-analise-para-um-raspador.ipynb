{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.9 - Traduzindo a análise para um raspador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta aula são abordados os seguintes temas:\n",
    "1. Seletores com Scrapy;\n",
    "2. Iteração com os elementos com *for*;\n",
    "3. Tradução da análise de página web para um raspador com Scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de começar, é preciso fazer um *fork* e *clone* do [repositório do Querido Diário](https://github.com/okfn-brasil/querido-diario). Depois, é preciso configurar o ambiente executando os códigos abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar um ambiente virtual\n",
    "python -m venv .venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ativar o ambiente virtual\n",
    ".venv/Scripts/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar as bibliotecas requeridas\n",
    "pip install -r data_collection/requirements-dev.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar o pré-commit, uma ferramenta que, ao fazer o commit do código, verifica se ele se adequa aos padrões do projeto\n",
    "pip install pre-commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para Depois de configurado o ambiente, criaremos um arquivo chamado *to_porto_nacional.py* dentro da pasta *data_collection/spiders*. Neste arquivo criaremos o raspador para a página do [**Diário Oficial de Porto Nacional (TO)**](https://diariooficial.portonacional.to.gov.br/edicoes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Seletores com Srapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ajustar a configuração inicial do rapspador, serão importadas as biblitecas usadas, bem como definindo a classe do mesmo utilizando os comandos abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime # biblioteca para manipular datas\n",
    "import re # biblioteca para manipular regex\n",
    "import scrapy\n",
    "from gazette.items import Gazette # \n",
    "from gazette.spiders.base import BaseGazetteSpider # importando classe básica para trabalhar o raspador\n",
    "\n",
    "class ToPortoNacionalSpider(BaseGazetteSpider): # definindo a classe para o raspador do diário oficial de Porto Nacional\n",
    "    name = \"to_porto_nacional\" # nome do raspador\n",
    "    TERRITORY_ID = \"1718204\" # id da cidade segundo o IBGE\n",
    "    allowed_domains = [\"diariooficial.portonacional.to.gov.br\"]\n",
    "    start_date = datetime.date(2017, 4, 3) # data inicial da raspagem\n",
    "    start_url = [\"https://www.diariooficial.portonacional.to.gov.br/edicoes\"] # url de partida do raspador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Iteração com os elementos com o for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dessa configuração inicial, iremos construir uma função que leia linha por linha da tabela de edições do Diário Oficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):  # definição da função parse que vai receber o url\n",
    "            \n",
    "    editions = response.xpath('//table[@class=\"table\"]/tbody/tr') # definição da variável que vai encontrar a tabela onde estão as edições do diário oficial\n",
    "\n",
    "    for edition in editions: # for para cada linha da tabela definida em editions\n",
    "        link = edition.xpath('./td[3]/div/a/@href').get() # recebe o link edição em pdf vinda do href\n",
    "        title = edition.xpath('./td[1]/strong/text()').get() # recebe o título da edição vinda do texto em strong\n",
    "        edition_number = re.search(r'EDIÇÃO Nº (\\d+)', title).group(1) # recebe o número da edição vindo da busca no título da edição\n",
    "        date_text = edition.xpath('./td[1]/br/following-sibling::text()').get()  # recebe a data vinda do texto após o br\n",
    "        day, month, year = re.match(r\"\\s+(\\d+) de (\\w+) de (\\d+)\\s+\", date_text).group(1, 2, 3) # recebe dia, mês e ano da busca feita na data\n",
    "        date = year+month+day # recebe a data vinda da junção de ano, mês e dia\n",
    "        date = datetime.datetime.strptime(date, \"%Y%B%d\").date() # transforma a data em datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tradução para o Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois, vamos guardar o que encontramos em cada linha, fazemos a comparação entre a data da edição raspada e a data de partida do raspador e utilizamos o tipo de retorno *yield* para persistir os dados utilizando a classe *Gazette*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if date < self.start_date: # compara a última data com a data inicial defininida como argumento da classe\n",
    "    return\n",
    "            \n",
    "yield Gazette( # retorna as informações abaixo em yield para cada linha da tabela\n",
    "    date=date,\n",
    "    edition_number=edition_number, \n",
    "    file_urls=[link], \n",
    "    power=\"executive\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, fazemos a paginação utilizando o XPath do link para próxima página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "next_page_link = response.xpath(\"//a[@rel=\"next\"]/@href\").get() # recebe a url da próxima página\n",
    "if next_page_link: # caso realmente exista o link\n",
    "    yield scrapy.Request( # faz nova requisição\n",
    "        url=response.urljoin(next_page_link), # coloca a próxima página na resposta\n",
    "        callback=self.parse, # inicia novamente o método parse\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
